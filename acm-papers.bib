@inproceedings{10.1145/3514094.3534203,
author = {Adam, Hammaad and Yang, Ming Ying and Cato, Kenrick and Baldini, Ioana and Senteio, Charles and Celi, Leo Anthony and Zeng, Jiaming and Singh, Moninder and Ghassemi, Marzyeh},
title = {Write It Like You See It: Detectable Differences in Clinical Notes by Race Lead to Differential Model Recommendations},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534203},
doi = {10.1145/3514094.3534203},
abstract = {Clinical notes are becoming an increasingly important data source for machine learning (ML) applications in healthcare. Prior research has shown that deploying ML models can perpetuate existing biases against racial minorities, as bias can be implicitly embedded in data. In this study, we investigate the level of implicit race information available to ML models and human experts and the implications of model-detectable differences in clinical notes. Our work makes three key contributions. First, we find that models can identify patient self-reported race from clinical notes even when the notes are stripped of explicit indicators of race. Second, we determine that human experts are not able to accurately predict patient race from the same redacted clinical notes. Finally, we demonstrate the potential harm of this implicit information in a simulation study, and show that models trained on these race-redacted clinical notes can still perpetuate existing biases in clinical treatment decisions.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {7–21},
numpages = {15},
keywords = {natural language processing, clinical notes, health equity},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534173,
author = {Akpinar, Nil-Jana and DiCiccio, Cyrus and Nandy, Preetam and Basu, Kinjal},
title = {Long-Term Dynamics of Fairness Intervention in Connection Recommender Systems},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534173},
doi = {10.1145/3514094.3534173},
abstract = {Recommender system fairness has been studied from the perspectives of a variety of stakeholders including content producers, the content itself and recipients of recommendations. Regardless of which type of stakeholders are considered, most works in this area assess the efficacy of fairness intervention by evaluating a single fixed fairness criterion through the lens of a one-shot, static setting. Yet recommender systems constitute dynamical systems with feedback loops from the recommendations to the underlying population distributions which could lead to unforeseen and adverse consequences if not taken into account. In this paper, we study a connection recommender system patterned after the systems employed by web-scale social networks and analyze the long-term effects of intervening on fairness in the recommendations. We find that, although seemingly fair in aggregate, common exposure and utility parity interventions fail to mitigate amplification of biases in the long term. We theoretically characterize how certain fairness interventions impact the bias amplification dynamics in a stylized Polya urn model.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {22–35},
numpages = {14},
keywords = {fairness, machine learning, recommender systems},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534152,
author = {Aleksandrov, Martin Damyanov},
title = {Dynamic Fleet Management and Household Feedback for Garbage Collection},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534152},
doi = {10.1145/3514094.3534152},
abstract = {We propose a solution for intelligent household garbage collection in smart cities. Garbage containers are assumed to be digitalized with Internet-of-Things sensors that are capable of sensing the fill levels of containers and transmitting this data through LoRaWAN networks to a central server. Data is used for dynamic fleet management and household feedback. We give a number of algorithms for these tasks. Fleet management requires scheduling containers for collections and assigning containers to trucks, as well as routing the trucks. Drivers receive such navigations via pervasive computing devices such as tablets, phones, or watches. Household feedback consists of information about the levels of generated garbage and the associated costs. Households receive this information on their home devices. Thus, unlike present solutions, our solution involves households in the intelligent collection of their garbage.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {36–45},
numpages = {10},
keywords = {Garbage Management, Vehicle Routing Problem, Ethics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534190,
author = {Balakrishnan, Sreejith and Bi, Jianxin and Soh, Harold},
title = {SCALES: From Fairness Principles to Constrained Decision-Making},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534190},
doi = {10.1145/3514094.3534190},
abstract = {This paper proposes SCALES, a general framework that translates well-established fairness principles into a common representation based on the Constraint Markov Decision Process (CMDP). With the help of causal language, our framework can place constraints on both the procedure of decision making (procedural fairness) as well as the outcomes resulting from decisions (outcome fairness). Specifically, we show that well-known fairness principles can be encoded either as a utility component, a non-causal component, or a causal component in a SCALES-CMDP. We illustrate SCALES using a set of case studies involving a simulated healthcare scenario and the real-world COMPAS dataset. Experiments demonstrate that our framework produces fair policies that embody alternative fairness principles in single-step and sequential decision-making scenarios.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {46–55},
numpages = {10},
keywords = {fairness, constrained reinforcement learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534145,
author = {Barnett, Julia and Diakopoulos, Nicholas},
title = {Crowdsourcing Impacts: Exploring the Utility of Crowds for Anticipating Societal Impacts of Algorithmic Decision Making},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534145},
doi = {10.1145/3514094.3534145},
abstract = {With the increasing pervasiveness of algorithms across industry and government, a growing body of work has grappled with how to understand their societal impact and ethical implications. Various methods have been used at different stages of algorithm development to encourage researchers and designers to consider the potential societal impact of their research. An understudied yet promising area in this realm is using participatory foresight to anticipate these different societal impacts. We employ crowdsourcing as a means of participatory foresight to uncover four different types of impact areas based on a set of governmental algorithmic decision making tools: (1) perceived valence, (2) societal domains, (3) specific abstract impact types, and (4) ethical algorithm concerns. Our findings suggest that this method is effective at leveraging the cognitive diversity of the crowd to uncover a range of issues. We further analyze the complexities within the interaction of the impact areas identified to demonstrate how crowdsourcing can illuminate patterns around the connections between impacts. Ultimately this work establishes crowdsourcing as an effective means of anticipating algorithmic impact which complements other approaches towards assessing algorithms in society by leveraging participatory foresight and cognitive diversity.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {56–67},
numpages = {12},
keywords = {anticipatory governance, broader impacts, ai ethics, thematic analysis},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534134,
author = {Ben Salem, Rim and A\"{\i}meur, Esma and Hage, Hicham},
title = {Aegis: An Agent for Multi-Party Privacy Preservation},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534134},
doi = {10.1145/3514094.3534134},
abstract = {The proliferation of social media set the foundation for the culture of over-disclosure where many people document every single event, incident, trip, etc. for everyone to see. Raising the individual's awareness of the privacy issues that they are subjecting themselves to can be challenging. This becomes more complex when the post being shared includes data "owned" by others. The existing approaches aiming to assist users in multi-party disclosure situations need to be revised to go beyond preferences to the "good" of the collective.This paper proposes an agent called Aegis to calculate the potential risk incurred by multi-party members in order to push privacy-preserving nudges to the sharer. Aegis is inspired by the consequentialist approach in normative ethical problem-solving techniques. The main contribution is the introduction of a social media-specific risk equation based on data valuation and the propagation of the post from intended to unintended audience. The proof-of-concept reports on how Aegis performs based on real-world data from the SNAP dataset and synthetically generated networks.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {68–77},
numpages = {10},
keywords = {normative ethical problem solving, consequentialist approach, nudges., aegis, data valuation},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534164,
author = {Bertrand, Astrid and Belloum, Rafik and Eagan, James R. and Maxwell, Winston},
title = {How Cognitive Biases Affect XAI-Assisted Decision-Making: A Systematic Review},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534164},
doi = {10.1145/3514094.3534164},
abstract = {The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI literature, structured around XAI-aided decision-making. We identify four main ways cognitive biases affect or are affected by XAI systems: 1) cognitive biases affect how XAI methods are designed, 2) they can distort how XAI techniques are evaluated in user studies, 3) some cognitive biases can be successfully mitigated by XAI techniques, and, on the contrary, 4) some cognitive biases can be exacerbated by XAI techniques. We construct this heuristic map through the systematic review of 37 papers-drawn from a corpus of 285-that reveal cognitive biases in XAI systems, including the explainability method and the user and task types in which they arise. We use the findings from our review to structure directions for future XAI systems to better align with people's cognitive processes.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {78–91},
numpages = {14},
keywords = {cognitive bias, xai., human-centered ai, explainable ai, explainability},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534195,
author = {Bessen, James and Impink, Stephen Michael and Seamans, Robert},
title = {The Cost of Ethical AI Development for AI Startups},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534195},
doi = {10.1145/3514094.3534195},
abstract = {Artificial Intelligence startups use training data as direct inputs in product development. These firms must balance numerous tradeoffs between ethical issues and data access without substantive guidance from regulators or existing judicial precedence. We survey these startups to determine what actions they have taken to address these ethical issues and the consequences of those actions. We find that 58% of these startups have established a set of AI principles. Startups with data-sharing relationships with high-technology firms or that have prior experience with privacy regulations are more likely to establish ethical AI principles and are more likely to take costly steps, like dropping training data or turning down business, to adhere to their ethical AI policies. Moreover, startups with ethical AI policies are more likely to invest in unconscious bias training, hire ethnic minorities and female programmers, seek expert advice, and search for more diverse training data. Potential costs associated with data-sharing relationships and the adherence to ethical policies may create tradeoffs between increased AI product competition and more ethical AI production.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {92–106},
numpages = {15},
keywords = {AI, scale barriers, startups, ethics, data},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534158,
author = {Bringas Colmenarejo, Alejandra and Nannini, Luca and Rieger, Alisa and Scott, Kristen M. and Zhao, Xuan and Patro, Gourab K and Kasneci, Gjergji and Kinder-Kurlanda, Katharina},
title = {Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534158},
doi = {10.1145/3514094.3534158},
abstract = {With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {107–118},
numpages = {12},
keywords = {standardization, utilitarian welfare, localization, ai regulation, deontological ethics, consequential ethics, eu ai proposal, egalitarian welfare},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534146,
author = {Bucknall, Benjamin S. and Dori-Hacohen, Shiri},
title = {Current and Near-Term AI as a Potential Existential Risk Factor},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534146},
doi = {10.1145/3514094.3534146},
abstract = {There is a substantial and ever-growing corpus of evidence and literature exploring the impacts of Artificial intelligence (AI) technologies on society, politics, and humanity as a whole. A separate, parallel body of work has explored existential risks to humanity, including but not limited to that stemming from unaligned Artificial General Intelligence (AGI). In this paper, we problematise the notion that current and near-term artificial intelligence technologies have the potential to contribute to existential risk by acting as intermediate risk factors, and that this potential is not limited to the unaligned AGI scenario. We propose the hypothesis that certain already-documented effects of AI can act as existential risk factors, magnifying the likelihood of previously identified sources of existential risk. Moreover, future developments in the coming decade hold the potential to significantly exacerbate these risk factors, even in the absence of artificial general intelligence. Our main contribution is a (non-exhaustive) exposition of potential AI risk factors and the causal relationships between them, focusing on how AI can affect power dynamics and information security. This exposition demonstrates that there exist causal pathways from AI systems to existential risks that do not presuppose hypothetical future AI capabilities.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {119–129},
numpages = {11},
keywords = {ai safety, societal impacts of ai, existential risk},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534184,
author = {Butcher, Bradley and Robinson, Chris and Zilka, Miri and Fogliato, Riccardo and Ashurst, Carolyn and Weller, Adrian},
title = {Racial Disparities in the Enforcement of Marijuana Violations in the US},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534184},
doi = {10.1145/3514094.3534184},
abstract = {Racial disparities in US drug arrest rates have been observed for decades, but their causes and policy implications are still contested. Some have argued that the disparities largely reflect differences in drug use between racial groups, while others have hypothesized that discriminatory enforcement policies and police practices play a significant role. In this work, we analyze racial disparities in the enforcement of marijuana violations in the US. Using data from the National Incident-Based Reporting System (NIBRS) and the National Survey on Drug Use and Health (NSDUH) programs, we investigate whether marijuana usage and purchasing behaviors can explain the racial composition of offenders in police records. We examine potential driving mechanisms behind these disparities and the extent to which county-level socioeconomic factors are associated with corresponding disparities. Our results indicate that the significant racial disparities in reported incidents and arrests cannot be explained by differences in marijuana days-of-use alone. Variations in the location where marijuana is purchased and in the frequency of these purchases partially explain the observed disparities. We observe an increase in racial disparities across most counties over the last decade, with the greatest increases in states that legalized the use of marijuana within this timeframe. Income, high school graduation rate, and rate of employment positively correlate with larger racial disparities, while the rate of incarceration is negatively correlated. We conclude with a discussion of the implications of the observed racial disparities in the context of algorithmic fairness.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {130–143},
numpages = {14},
keywords = {marijuana, law enforcement, racial disparities},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534160,
author = {Cachel, Kathleen and Rundensteiner, Elke},
title = {FINS Auditing Framework: Group Fairness for Subset Selections},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534160},
doi = {10.1145/3514094.3534160},
abstract = {Subset selection is an integral component of AI systems that is increasingly affecting people's livelihoods in applications ranging from hiring, healthcare, education, to financial decisions. Subset selections powered by AI-based methods include top-k analytics, data summarization, clustering, and multi-winner voting. While group fairness auditing tools have been proposed for classification systems, these state-of-the-art tools are not directly applicable to measuring and conceptualizing fairness in selected subsets. In this work, we introduce the first comprehensive auditing framework, FINS, to support stakeholders in interpretably quantifying group fairness across a diverse range of subset-specific fairness concerns. FINS offers a family of novel measures that provide a flexible means to audit group fairness for fairness goals ranging from item-based, score-based, and a combination thereof. FINS provides one unified easy-to-understand interpretation across these different fairness problems. Further, we develop guidelines through the FINS Fair Subset Chart, that supports auditors in determining which measures are relevant to their problem context and fairness objectives. We provide a comprehensive mapping between each fairness measure and the belief system (i.e., worldview) that is encoded within its measurement of fairness. Lastly, we demonstrate the interpretability and efficacy of FINS in supporting the identification of real bias with case studies using AirBnB listings and voter records.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {144–155},
numpages = {12},
keywords = {machine learning fairness, algorithmic fairness, subset selection},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534162,
author = {Caliskan, Aylin and Ajay, Pimparkar Parth and Charlesworth, Tessa and Wolfe, Robert and Banaji, Mahzarin R.},
title = {Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534162},
doi = {10.1145/3514094.3534162},
abstract = {Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men). Although efforts have been made to mitigate bias in word embeddings, with the hope of improving fairness in downstream Natural Language Processing (NLP) applications, these efforts will remain limited until we more deeply understand the multiple (and often subtle) ways that social biases can be reflected in word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). While some previous research has helped uncover biases in specific semantic associations between a group and a target domain (e.g., women - family), using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. We leave the analysis of non-binary gender to future work due to the challenges in accurate group representation caused by limitations inherent in data.First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a ~20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence. Ultimately, these findings move the study of gender bias in word embeddings beyond the basic investigation of semantic relationships to also study gender differences in multiple manifestations in text. Given the central role of word embeddings in NLP applications, it is essential to more comprehensively document where biases exist and may remain hidden, allowing them to persist without our awareness throughout large text corpora.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {156–170},
numpages = {15},
keywords = {ai bias, representation, psycholinguistics, gender bias, word embeddings, masculine default},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534182,
author = {Canavotto, Ilaria and Horty, John},
title = {Piecemeal Knowledge Acquisition for Computational Normative Reasoning},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534182},
doi = {10.1145/3514094.3534182},
abstract = {We present a hybrid approach to knowledge acquisition and representation for machine ethics---or more generally, computational normative reasoning. Building on recent research in artificial intelligence and law, our approach is modeled on the familiar practice of decision-making under precedential constraint in the common law. We first provide a formal characterization of this practice, showing how a body of normative information can be constructed in a way that is piecemeal, distributed, and responsive to particular circumstances. We then discuss two possible applications: first, a robot childminder, and second, moral judgment in a bioethical domain.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {171–180},
numpages = {10},
keywords = {machine ethics, AI and law, computational normative reasoning, knowledge acquisition/representation},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534180,
author = {Chen, Mo and Engelmann, Severin and Grossklags, Jens},
title = {Ordinary People as Moral Heroes and Foes: Digital Role Model Narratives Propagate Social Norms in China's Social Credit System},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534180},
doi = {10.1145/3514094.3534180},
abstract = {The Chinese Social Credit System (SCS) is a digital sociotechnical credit system that rewards and sanctions economic and social behaviors of individuals and companies. As a complex and transformative digital credit system, the SCS uses digital communication channels to inform the Chinese public about behaviors that lead to reward or sanction. Since 2017, the Chinese government has been publishing "blameworthy" and "praiseworthy" role model narratives of ordinary Chinese citizens on its central SCS information platform creditchina.gov.cn. Across many cultures, role model narratives are a known instrument to convey "appropriate" and "inappropriate" social norms. Using a directed content analysis methodology, we study the SCS-specific social norms embedded in 100 "praiseworthy" and 100 "blameworthy" role model narratives published on creditchina.gov.cn. "Blameworthy" role model narratives stress social norms associated with an "immoral" SCS identity label termed "Lao Lai" - a "moral foe" that fails to repay debt. SCS role model narratives familiarize Chinese society with SCS-specific measures such as digital surveillance, public shaming, and disproportionate punishment. Our study makes progress towards understanding how a state-run sociotechnical credit system combines digital tools with culturally familiar customs to propagate "blameworthy" and "praiseworthy" identities.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {181–191},
numpages = {11},
keywords = {china, moral education, social credit system, narrative study},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534131,
author = {Clarke, Sam and Whittlestone, Jess},
title = {A Survey of the Potential Long-Term Impacts of AI: How AI Could Lead to Long-Term Changes in Science, Cooperation, Power, Epistemics and Values},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534131},
doi = {10.1145/3514094.3534131},
abstract = {It is increasingly recognised that advances in artificial intelligence could have large and long-lasting impacts on society. However, what form those impacts will take, just how large and long-lasting they will be, and whether they will ultimately be positive or negative for humanity, is far from clear. Based on surveying literature on the societal impacts of AI, we identify and discuss five potential long-term impacts of AI: how AI could lead to long-term chances in science, cooperation, power, epistemics, and values. We review the state of existing research in each of these areas and highlight priority questions for future research.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {192–202},
numpages = {11},
keywords = {foresight, societal impacts of ai, transformative ai, cooperation, epistemic processes, power and inequality, scientific progress, ai alignment, conflict},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534159,
author = {Dai, Jessica and Upadhyay, Sohini and Aivodji, Ulrich and Bach, Stephen H. and Lakkaraju, Himabindu},
title = {Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post Hoc Explanations},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534159},
doi = {10.1145/3514094.3534159},
abstract = {As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across all subgroups of a population. For instance, it should not be the case that explanations associated with instances belonging to, e.g., women, are less accurate than those associated with other genders. In this work, we initiate the study of identifying group-based disparities in explanation quality. To this end, we first outline several key properties that contribute to explanation quality-namely, fidelity (accuracy), stability, consistency, and sparsity-and discuss why and how disparities in these properties can be particularly problematic. We then propose an evaluation framework which can quantitatively measure disparities in the quality of explanations. Using this framework, we carry out an empirical analysis with three datasets, six post hoc explanation methods, and different model classes to understand if and when group-based disparities in explanation quality arise. Our results indicate that such disparities are more likely to occur when the models being explained are complex and non-linear. We also observe that certain post hoc explanation methods (e.g., Integrated Gradients, SHAP) are more likely to exhibit disparities. Our work sheds light on previously unexplored ways in which explanation methods may introduce unfairness in real world decision making.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {203–214},
numpages = {12},
keywords = {explainable machine learning, robustness, fairness, interpretability},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534144,
author = {Dai, Xinyue and Keane, Mark T. and Shalloo, Laurence and Ruelle, Elodie and Byrne, Ruth M.J.},
title = {Counterfactual Explanations for Prediction and Diagnosis in XAI},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534144},
doi = {10.1145/3514094.3534144},
abstract = {We compared two sorts of explanations for decisions made by an AI system: counterfactual explanations about how an outcome could have been different in the past, and prefactual explanations about how it could be different in the future. We examined the effects of these alternative explanation strategies on the accuracy of users' judgments about the AI app's predictions about an outcome (inferred from information about the causes), compared to the accuracy of their judgments about the app's diagnoses of a cause (inferred from information about the outcome). The tasks were based on a simulated SmartAgriculture decision support system for grass growth outcomes on dairy farms in Experiment 1, and for an analogous alien planet domain in Experiment 2. The two experiments, with 243 participants, also tested users' confidence in their decisions, and their satisfaction with the explanations. Users made more accurate diagnoses of the presence of causes based on information about their outcome, compared to predictions of an outcome given information about the presence of causes. Their predictions and diagnoses were helped equally by counterfactual explanations and prefactual ones.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {215–226},
numpages = {12},
keywords = {counterfactual, diagnosis, explainable ai (xai), prediction, explanation, smart agriculture},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534187,
author = {Deshpande, Advait and Sharp, Helen},
title = {Responsible AI Systems: Who Are the Stakeholders?},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534187},
doi = {10.1145/3514094.3534187},
abstract = {As of 2021, there were more than 170 guidelines on AI ethics and responsible, trustworthy AI in circulation according to the AI Ethics Guidelines Global Inventory maintained by AlgorithmWatch, an organisation which tracks the effects of increased digitalisation on everyday lives. However, from the perspective of day-to-day work, for those engaged in designing, developing, and maintaining AI systems identifying relevant guidelines and translating them into practice presents a challenge.The aim of this paper is to help anyone engaged in building a responsible AI system by identifying an indicative long-list of potential stakeholders. This list of impacted stakeholders is intended to enable such AI system builders to decide which guidelines are most suited to their practice. The paper draws on a literature review of articles short-listed based on searches conducted in the ACM Digital Library and Google Scholar. The findings are based on content analysis of the short-listed literature guided by probes which draw on the ISO 26000:2010 Guidance on social responsibility.The paper identifies three levels of potentially relevant stakeholders when responsible AI systems are considered: individual stakeholders (including users, developers, and researchers), organisational stakeholders, and national / international stakeholders engaged in making laws, rules, and regulations. The main intended audience for this paper is software, requirements, and product engineers engaged in building AI systems. In addition, business executives, policy makers, legal/regulatory experts, AI researchers, public, private, and third sector organisations developing responsible AI guidelines, and anyone interested in seeing functional responsible AI systems are the other intended audience for this paper.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {227–236},
numpages = {10},
keywords = {responsible AI systems, ISO 26000:2010 guidance on social responsibility, corporate social responsibility, AI system builders, stakeholder identification, AI ethics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534151,
author = {Drage, Eleanor and Mackereth, Kerry},
title = {Does AI De-Bias Recruitment? Race, Gender, and AI's 'Eradication of Differences Between Groups'},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534151},
doi = {10.1145/3514094.3534151},
abstract = {In this paper, we analyze two key claims offered by recruitment AI companies in relation to the development and deployment of AI-powered HR tools: 1) recruitment AI can objectively assess candidates by removing gender and race from their systems, and 2) this removal of gender and race will make recruitment fairer, help customers attain their DEI goals, and lay the foundations for a truly meritocratic culture to thrive within an organization. We argue that these claims are misleading for four reasons: First, attempts to 'strip' gender and race from AI systems often misunderstand what gender and race are, casting them as isolatable attributes rather than broader systems of power. Second, the attempted outsourcing of 'diversity work' to AI-powered hiring tools may unintentionally entrench cultures of inequality and discrimination by failing to address the systemic problems within organizations. Third, AI hiring tools' supposedly neutral assessment of candidates' traits belies the power relationship between the observer and the observed. Specifically, the racialized history of character analysis and its associated processes of classification and categorisation play into longer histories of taxonomical sorting and reflect the current demands and desires of the job market, even when not explicitly conducted along the lines of gender and race. Fourth, recruitment AI tools help produce the 'ideal candidate' that they supposedly identify through by constructing associations between words and people's bodies. From these four conclusions outlined above, we offer three key recommendations to AI HR firms, their customers, and policy makers going forward.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {237},
numpages = {1},
keywords = {recruitment, gender, bias, race, ai ethics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534192,
author = {Engelmann, Severin and Scheibe, Valentin and Battaglia, Fiorella and Grossklags, Jens},
title = {Social Media Profiling Continues to Partake in the Development of Formalistic Self-Concepts. Social Media Users Think So, Too.},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534192},
doi = {10.1145/3514094.3534192},
abstract = {Social media platforms generate user profiles to recommend informational resources including targeted advertisements. The technical possibilities of user profiling methods go beyond the classification of individuals into types of potential customers. They enable the transformation of implicit identity claims of individuals into explicit declarations of identity. As such, a key ethical challenge of social media profiling is that it stands in contrast with people's ability to self-determine autonomously, a core principle of the right to informational self-determination.In this research study, we take a step back and revisit theories of personal identity in philosophy that underline two constitutive meta-principles necessary for individuals to self-interpret autonomously: justification and control. That is, individuals have the ability to justify and control essential aspects of their self-concept. Returning to a philosophical basis for the value of self-determination serves as a reminder that user profiling is essentially normative in that it formalizes a person's self-concept within an algorithmic system. To understand whether social media users would want to justify and control social media's identity declarations, we conducted a vignette survey study (N = 368). First, participants indicate a strong preference for more transparency in social media identity declarations, a core requirement for the justification of a self-concept. Second, respondents state they would correct wrong identity declarations but show no clear motivation to manage them. Finally, our results illustrate that social media users acknowledge the narrative force of social media profiling but do not strongly believe in its capacity to shape their self-concept.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {238–252},
numpages = {15},
keywords = {social media, personal identity, ethics of artificial intelligence, autonomy, user profiling},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534193,
author = {Evans, Charles and Benn, Claire and Ojea Quintana, Ignacio and Robinson, Pamela and Thi\'{e}baux, Sylvie},
title = {Stochastic Policies in Morally Constrained (C-)SSPs},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534193},
doi = {10.1145/3514094.3534193},
abstract = {Stochastic policies often outperform deterministic ones. This is especially true for Constrained Stochastic Shortest Path (C-SSP) problems, a popular approach to planning under uncertainty with multiple objectives. Nevertheless, there are moral concerns about stochastic policies that should deter us from selecting them. In this paper, we identify some of these moral concerns and offer 'acceptability constraints' that allow only certain stochastic policies to be selected. We propose a novel C-SSP solver able to integrate our moral acceptability constraints, we evaluate its performance in a relevant test problem, and we show that our approach can successfully produce acceptable policies in morally significant domains.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {253–264},
numpages = {12},
keywords = {risk of harm, stochastic policies, ethical decision making, moral constraints, constrained stochastic shortest path problems, uncertainty, automated planning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534137,
author = {Franklin, Jade S. and Bhanot, Karan and Ghalwash, Mohamed and Bennett, Kristin P. and McCusker, Jamie and McGuinness, Deborah L.},
title = {An Ontology for Fairness Metrics},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534137},
doi = {10.1145/3514094.3534137},
abstract = {Recent research has revealed that many machine-learning models and the datasets they are trained on suffer from various forms of bias, and a large number of different fairness metrics have been created to measure this bias. However, determining which metrics to use, as well as interpreting their results, is difficult for a non-expert due to a lack of clear guidance and issues of ambiguity or alternate naming schemes between different research papers. To address this knowledge gap, we present the Fairness Metrics Ontology (FMO), a comprehensive and extensible knowledge resource that defines each fairness metric, describes their use cases, and details the relationships between them. We include additional concepts related to fairness and machine learning models, enabling the representation of specific fairness information within a resource description framework (RDF) knowledge graph. We evaluate the ontology by examining the process of how reasoning-based queries to the ontology were used to guide the fairness metric-based evaluation of a synthetic data model.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {265–275},
numpages = {11},
keywords = {machine learning evaluation, rdf knowledge graph, bias, fairness metric},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534140,
author = {Franklin, Matija and Ashton, Hal and Awad, Edmond and Lagnado, David},
title = {Causal Framework of Artificial Autonomous Agent Responsibility},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534140},
doi = {10.1145/3514094.3534140},
abstract = {Recent empirical work on people's attributions of responsibility toward artificial autonomous agents (such as Artificial Intelligence agents or robots) has delivered mixed findings. The conflicting results reflect differences in context, the roles of AI and human agents, and the domain of application. In this article, we outline a causal framework of responsibility attribution which integrates these findings. It outlines nine factors that influence responsibility attribution - causality, role, knowledge, objective foreseeability, capability, intent, desire, autonomy, and character. We propose a framework of responsibility that outlines the causal relationships between the nine factors and responsibility. To empirically test the framework we discuss some initial findings and outline an approach to using serious games for causal cognitive research on responsibility attribution. Specifically, we propose a game that uses a generative approach to creating different scenarios, in which participants can freely inspect different sources of information to make judgments about human and artificial autonomous agents.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {276–284},
numpages = {9},
keywords = {attribution, causal cognition, blame, responsibility},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534183,
author = {Garcia de Macedo, Maysa Malfiza and Clarke, Wyatt and Lucherini, Eli and Baldwin, Tyler and Queiroz Neto, Dilermando and de Paula, Rogerio Abreu and Das, Subhro},
title = {Practical Skills Demand Forecasting via Representation Learning of Temporal Dynamics},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534183},
doi = {10.1145/3514094.3534183},
abstract = {Rapid technological innovation threatens to leave much of the global workforce behind. Today's economy juxtaposes white-hot demand for skilled labor against stagnant employment prospects for workers unprepared to participate in a digital economy. It is a moment of peril and opportunity for every country, with outcomes measured in long-term capital allocation and the life satisfaction of billions of workers. To meet the moment, governments and markets must find ways to quicken the rate at which the supply of skills reacts to changes in demand. More fully and quickly understanding labor market intelligence is one route. In this work, we explore the utility of time series forecasts to enhance the value of skill demand data gathered from online job advertisements. This paper presents a pipeline which makes one-shot multi-step forecasts into the future using a decade of monthly skill demand observations based on a set of recurrent neural network methods. We compare the performance of a multivariate model versus a univariate one, analyze how correlation between skills can influence multivariate model results, and present predictions of demand for a selection of skills practiced by workers in the information technology industry.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {285–294},
numpages = {10},
keywords = {skill representation, labor economics, neural networks, demand forecasting},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534201,
author = {Gemalmaz, Meric Altug and Yin, Ming},
title = {Understanding Decision Subjects' Fairness Perceptions and Retention in Repeated Interactions with AI-Based Decision Systems},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534201},
doi = {10.1145/3514094.3534201},
abstract = {The wide application of AI-based decision systems in many high-stake domains has raised concerns regarding fairness of these systems. As these systems will lead to real-life consequences to people who are subject to their decisions, understanding what these decision subjects perceive as a fair or unfair system is of vital importance. In this paper, we extend prior work in this direction by taking a perspective of repeated interactions---We ask that when decision subjects interact with an AI-based decision system repeatedly and can strategically respond to the system by determining whether to stay in the system, what factors will affect the decision subjects' fairness perceptions and retention in the system and how. To answer these questions, we conducted two randomized human-subject experiments in the context of an AI-based loan lending system. Our results suggest that in repeated interactions with the AI-based decision system, overall, decision subjects' fairness perceptions and retention in the system are significantly affected by whether the system is in favor of the group that subjects themselves belong to, rather than whether the system treats different groups in an unbiased way. However, decision subjects with different qualification levels have different reactions to the AI system's biased treatment across groups or the AI system's tendency to favor/disfavor their own group. Finally, we also find that while subjects' retention in the AI-based decision system is largely driven by their own prospects of receiving the favorable decision from the system, their fairness perceptions of the system is influenced by the system's treatment to people in other groups in a complex way.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {295–306},
numpages = {12},
keywords = {human-AI interaction, fairness perceptions, fairness, AI-based decision systems, retention, human-subject experiments},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534157,
author = {Ghosh, Avijit and Shanbhag, Aalok and Wilson, Christo},
title = {FairCanary: Rapid Continuous Explainable Fairness},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534157},
doi = {10.1145/3514094.3534157},
abstract = {Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems.We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing explanations computed for each individual prediction to quickly compute explanations for the QDD bias metrics. This optimization makes FairCanary an order of magnitude faster than previous work that has tried to generate feature-level bias explanations.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {307–316},
numpages = {10},
keywords = {continuous measurement, fairness, model explanation, drift},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534172,
author = {He, Yuzi and Burghardt, Keith and Guo, Siyi and Lerman, Kristina},
title = {Learning Fairer Interventions},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534172},
doi = {10.1145/3514094.3534172},
abstract = {Explicit and implicit bias clouds human judgment, leading to discriminatory treatment of disadvantaged groups. A fundamental goal of automated decisions is to avoid the pitfalls in human judgment by developing decision strategies that can be applied to all protected groups. Improving fairness of interventions via automated decision-inspired methods, however, has been under-utilized. In this paper, we propose a causal framework that learns optimal intervention policies from data subject to novel fairness constraints. We define two measures of treatment bias and infer treatment assignments that minimize the bias against protected groups while optimizing overall outcomes. We demonstrate the existence of trade-offs when balancing fairness and overall benefit; however, allowing preferential treatment of protected groups in certain circumstances (affirmative action) can dramatically improve the overall benefit while also preserving fairness. We apply our framework to data containing outcomes on standardized tests and show how it can be used to design real-world policies that fairly improve academic performance for different geographic areas. Our framework provides a principled way to learn fair treatment policies in real-world settings.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {317–323},
numpages = {7},
keywords = {fairness metrics, treatment biases, policy recommendations, fair treatment, affirmative action},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534149,
author = {Helm, Paula and Michael, Loizos and Schelenz, Laura},
title = {Diversity by Design? Balancing the Inclusion and Protection of Users in an Online Social Platform},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534149},
doi = {10.1145/3514094.3534149},
abstract = {The unreflected promotion of diversity as a value in social interactions --- including in technology-mediated ones --- risks emphasizing the benefits of inclusion without recognizing the potential harm of failing to protect vulnerable individuals or account for the empowerment of marginalized groups. Adopting the position that technology is not value-neutral, we seek to answer the question of how technology-mediated social platforms can accommodate diversity by design by balancing the often tension-ridden principles of protection and inclusion. In this paper, we present our research program, developed strategy, as well as first analyses and results. Building on approaches from scenario analysis and Value Sensitive Design, we identify key arguments for a "diversity by design''-agenda. Furthermore, we discuss how these arguments can be operationalized and implemented in a diversity-aware chatbot and provide a critical reflection on the limits and drawbacks of the proposed approach.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {324–334},
numpages = {11},
keywords = {platform technology, diversity, inclusion, social media, ethics, design, intervention, norms, content moderation, protection, matching algorithms},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534196,
author = {Hullman, Jessica and Kapoor, Sayash and Nanayakkara, Priyanka and Gelman, Andrew and Narayanan, Arvind},
title = {The Worst of Both Worlds: A Comparative Analysis of Errors in Learning from Data in Psychology and Machine Learning},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534196},
doi = {10.1145/3514094.3534196},
abstract = {Arguments that machine learning (ML) is facing a reproducibility and replication crisis suggest that some published claims in research cannot be taken at face value. Concerns inspire analogies to the replication crisis affecting the social and medical sciences. A deeper understanding of what reproducibility concerns in supervised ML research have in common with the replication crisis in experimental science puts the new concerns in perspective, and helps researchers avoid "the worst of both worlds," where ML researchers begin borrowing methodologies from explanatory modeling without understanding their limitations and vice versa. We contribute a comparative analysis of concerns about inductive learning that arise in causal attribution as exemplified in psychology versus predictive modeling as exemplified in ML. We identify common themes in reform discussions, like overreliance on asymptotic theory and non-credible beliefs about real-world data generating processes. We argue that in both fields, claims from learning are implied to generalize outside the specific environment studied (e.g., the input dataset or subject sample, modeling implementation, etc.) but are often difficult to refute due to underspecification of key parts of the learning pipeline. We conclude by discussing risks that arise when sources of errors are misdiagnosed and the need to acknowledge the role of human inductive biases in learning and reform.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {335–348},
numpages = {14},
keywords = {generalizability, replication, machine learning, science reform},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534188,
author = {Kasirzadeh, Atoosa},
title = {Algorithmic Fairness and Structural Injustice: Insights from Feminist Political Philosophy},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534188},
doi = {10.1145/3514094.3534188},
abstract = {Data-driven predictive algorithms are widely used to automate and guide high-stake decision making such as bail and parole recommendation, medical resource distribution, and mortgage allocation. Nevertheless, harmful outcomes biased against vulnerable groups have been reported. The growing research field known as 'algorithmic fairness' aims to mitigate these harmful biases. Its primary methodology consists in proposing mathematical metrics to address the social harms resulting from an algorithm's biased outputs. The metrics are typically motivated by -- or substantively rooted in -- ideals of distributive justice, as formulated by political and legal philosophers. The perspectives of feminist political philosophers on social justice, by contrast, have been largely neglected. Some feminist philosophers have criticized the local scope of the paradigm of distributive justice and have proposed corrective amendments to surmount its limitations. The present paper brings some key insights of feminist political philosophy to algorithmic fairness. The paper has three goals. First, I show that algorithmic fairness does not accommodate structural injustices in its current scope. Second, I defend the relevance of structural injustices -- as pioneered in the contemporary philosophical literature by Iris Marion Young -- to algorithmic fairness. Third, I take some steps in developing the paradigm of 'responsible algorithmic fairness' to correct for errors in the current scope and implementation of algorithmic fairness. I close by some reflections of directions for future research.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {349–356},
numpages = {8},
keywords = {ethical machine learning, responsibility, feminist philosophy, distributive justice, algorithmic fairness, ethics of artificial intelligence, political philosophy, structural injustice, algorithmic justice, algorithmic bias},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534154,
author = {Kumar, I. Elizabeth and Hines, Keegan E. and Dickerson, John P.},
title = {Equalizing Credit Opportunity in Algorithms: Aligning Algorithmic Fairness Research with U.S. Fair Lending Regulation},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534154},
doi = {10.1145/3514094.3534154},
abstract = {Credit is an essential component of financial wellbeing in America, and unequal access to it is a large factor in the economic disparities between demographic groups that exist today. Today, machine learning algorithms, sometimes trained on alternative data, are increasingly being used to determine access to credit, yet research has shown that machine learning can encode many different versions of "unfairness," thus raising the concern that banks and other financial institutions could---potentially unwittingly---engage in illegal discrimination through the use of this technology. In the US, there are laws in place to make sure discrimination does not happen in lending and agencies charged with enforcing them. However, conversations around fair credit models in computer science and in policy are often misaligned: fair machine learning research often lacks legal and practical considerations specific to existing fair lending policy, and regulators have yet to issue new guidance on how, if at all, credit risk models should be utilizing practices and techniques from the research community. This paper aims to better align these sides of the conversation. We describe the current state of credit discrimination regulation in the United States, contextualize results from fair ML research to identify the specific fairness concerns raised by the use of machine learning in lending, and discuss regulatory opportunities to address these concerns.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {357–368},
numpages = {12},
keywords = {fair lending law, ecoa, machine learning, fair machine learning, algorithmic fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534141,
author = {Kusuma, Manisha and Mohanty, Vikram and Wang, Marx and Luther, Kurt},
title = {Civil War Twin: Exploring Ethical Challenges in Designing an Educational Face Recognition Application},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534141},
doi = {10.1145/3514094.3534141},
abstract = {Facial recognition systems pose numerous ethical challenges around privacy, racial and gender bias, and accuracy, yet little guidance is available for designers and developers. We explore solutions to these challenges in a three-phase design process to create Civil War Twin (CWT), an educational web-based application where users can discover their lookalikes from the American Civil War era (1861--65) while learning more about facial recognition and history. Through this design process, we operationalize a framework for AI literacy, consult with scholars of history, gender, and race, and evaluate CWT in feedback sessions with diverse prospective users. We iteratively formulate design goals to incorporate transparency, inclusivity, speculative design, and empathy into our application. We found that users' perceived learning about the strengths and limitations of facial recognition and Civil War history improved after using CWT, and that our design successfully met users' ethical standards. We also discuss how our ethical design process can be applied to future facial recognition applications.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {369–384},
numpages = {16},
keywords = {facial recognition, ai literacy, digital history, ethical design, education, human-ai interaction},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534167,
author = {Langenkamp, Max and Yue, Daniel N.},
title = {How Open Source Machine Learning Software Shapes AI},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534167},
doi = {10.1145/3514094.3534167},
abstract = {If we want a future where AI serves a plurality of interests, then we should pay attention to the factors that drive its success. While others have studied the importance of data, hardware, and models in directing the trajectory of AI, we argue that open source software is a neglected factor shaping AI as a discipline. We start with the observation that almost all AI research and applications are built on machine learning open source software (MLOSS). This paper presents three contributions. First, it quantifies the outsized impact of MLOSS by using Github contributions data. By contrasting the costs of MLOSS and its economic benefits, we find that the average dollar of MLOSS investment corresponds to at least $100 of global economic value created, corresponding to $30B of economic value created this year. Second, we leverage interviews with AI researchers and developers to develop a causal model of the effect of open sourcing on economic value. We argue that open sourcing creates value through three primary mechanisms: standardization of MLOSS tools, increased experimentation in AI research, and creation of communities. Finally, we consider the incentives for developing MLOSS and the broader implications of these effects. We intend this paper to be useful for technologists and academics who want to analyze and critique AI, and policymakers who want to better understand and regulate AI systems.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {385–395},
numpages = {11},
keywords = {open source, machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534147,
author = {Li, Nianyun and Goel, Naman and Ash, Elliott},
title = {Data-Centric Factors in Algorithmic Fairness},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534147},
doi = {10.1145/3514094.3534147},
abstract = {Notwithstanding the widely held view that data generation and data curation processes are prominent sources of bias in machine learning algorithms, there is little empirical research seeking to document and understand the specific data dimensions affecting algorithmic unfairness. Contra the previous work, which has focused on modeling using simple, small-scale benchmark datasets, we hold the model constant and methodically intervene on relevant dimensions of a much larger, more diverse dataset. For this purpose, we introduce a new dataset on recidivism in 1.5 million criminal cases from courts in the U.S. state of Wisconsin, 2000-2018. From this main dataset, we generate multiple auxiliary datasets to simulate different kinds of biases in the data. Focusing on algorithmic bias toward different race/ethnicity groups, we assess the relevance of training data size, base rate difference between groups, representation of groups in the training data, temporal aspects of data curation, including race/ethnicity or neighborhood characteristics as features, and training separate classifiers by race/ethnicity or crime type. We find that these factors often do influence fairness metrics holding the classifier specification constant, without having a corresponding effect on accuracy metrics. The methodology and the results in the paper provide a useful reference point for a data-centric approach to studying algorithmic fairness in recidivism prediction and beyond.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {396–410},
numpages = {15},
keywords = {recidivism prediction, datasets, algorithmic fairness, machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534142,
author = {Li, Zhuoyan and Lu, Zhuoran and Yin, Ming},
title = {Towards Better Detection of Biased Language with Scarce, Noisy, and Biased Annotations},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534142},
doi = {10.1145/3514094.3534142},
abstract = {Biased language is prevalent in today's online social media. To reduce the amount of online biased language, one critical first step is to accurately detect such biased language, ideally automatically. This is a challenging problem, however, as the annotated data necessary for training a biased language classifier is either scarce and costly (e.g., when collected from experts), or noisy and potentially biased on their own (e.g., when collected from crowd workers). The biased language classifier built based on these annotations may thus be inaccurate, and sometimes unfair (e.g., have systematic accuracy disparities across texts with different political leanings). In this paper, we propose a novel method, CLEARE, for biased language detection, in which we utilize self-supervised contrastive learning to enhance the biased language classifier---we learn a robust encoder of the textual data through solving a min-max optimization problem, so that the encoder could help achieve the best classification performance even if the worst data augmentation strategy is selected. Extensive evaluations suggest that CLEARE shows substantial improvements compared to the state-of-art biased language detection methods on several benchmark datasets, in terms of improving both the accuracy and the fairness of the detection.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {411–423},
numpages = {13},
keywords = {bias detection, fairness, biased language, contrastive learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534155,
author = {Liu, David and Nanayakkara, Priyanka and Sakha, Sarah Ariyan and Abuhamad, Grace and Blodgett, Su Lin and Diakopoulos, Nicholas and Hullman, Jessica R. and Eliassi-Rad, Tina},
title = {Examining Responsibility and Deliberation in AI Impact Statements and Ethics Reviews},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534155},
doi = {10.1145/3514094.3534155},
abstract = {The artificial intelligence research community is continuing to grapple with the ethics of its work by encouraging researchers to discuss potential positive and negative consequences. Neural Information Processing Systems (NeurIPS), a top-tier conference for machine learning and artificial intelligence research, first required a statement of broader impact in 2020. In 2021, NeurIPS updated their call for papers such that 1) the impact statement focused on negative societal impacts and was not required but encouraged, 2) a paper checklist and ethics guidelines were provided to authors, and 3) papers underwent ethics reviews and could be rejected on ethical grounds. In light of these changes, we contribute a qualitative analysis of 231 impact statements and all publicly-available ethics reviews. We describe themes arising around the ways in which authors express agency (or lack thereof) in identifying or mitigating negative consequences and assign responsibility for mitigating negative societal impacts. We also characterize ethics reviews in terms of the types of issues raised by ethics reviewers (falling into categories of policy-oriented and non-policy-oriented), recommendations ethics reviewers make to authors (e.g., in terms of adding or removing content), and interaction between authors, ethics reviewers, and original reviewers (e.g., consistency between issues flagged by original reviewers and those discussed by ethics reviewers). Finally, based on our analysis we make recommendations for how authors can be further supported in engaging with the ethical implications of their work.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {424–435},
numpages = {12},
keywords = {broader impact, impact statements, ai ethics, ethics review},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534139,
author = {Liu, Yuxin and Moore, Adam and Webb, Jamie and Vallor, Shannon},
title = {Artificial Moral Advisors: A New Perspective from Moral Psychology},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534139},
doi = {10.1145/3514094.3534139},
abstract = {Philosophers have recently put forward the possibility of achieving moral enhancement through artificial intelligence (e.g., Giubilini and Savulescu's version [32]), proposing various forms of "artificial moral advisor" (AMA) to help people make moral decisions without the drawbacks of human cognitive limitations. In this paper, we provide a new perspective on the AMA, drawing on empirical evidence from moral psychology to point out several challenges to these proposals that have been largely neglected by AI ethicists. In particular, we suggest that the AMA at its current conception is fundamentally misaligned with human moral psychology - it incorrectly assumes a static moral values framework underpinning the AMA's attunement to individual users, and people's reactions and subsequent (in)actions in response to the AMA suggestions will likely diverge substantially from expectations. As such, we note the necessity for a coherent understanding of human moral psychology in the future development of AMAs.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {436–445},
numpages = {10},
keywords = {artificial moral advisor, moral psychology, normative ethics, ai ethics, ai moral enhancement},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534125,
author = {Longoni, Chiara and Cian, Luca and Kyung, Ellie},
title = {Artificial Intelligence in the Government: Responses to Failures and Social Impact},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534125},
doi = {10.1145/3514094.3534125},
abstract = {Artificial Intelligence (AI) is pervading the government and transforming how public services are provided to consumers---from allocation of benefits to law enforcement, risk monitoring and the provision of services. Despite technological improvements, AI systems are fallible and may err. How do consumers respond when learning of AI's failures? In thirteen preregistered studies (N = 3,724), we document a robust effect of algorithmic transference: algorithmic failures are generalized more broadly than human failures. Rather than reflecting generalized algorithm aversion, algorithmic transference is rooted in social categorization: it stems from how people perceive a group of AI systems versus a group of humans---as outgroups characterized by greater homogeneity than ingroups of comparable humans. Because AI systems are perceived as more homogeneous than people, failure information about one AI algorithm is transferred to another algorithm at a higher rate than failure information about a person is transferred to another person. Assessing AI's impact on consumers and societies, we show how the premature or mismanaged deployment of faulty AI technologies may engender algorithmic transference and undermine the very institutions that AI systems are meant to modernize.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {446},
numpages = {1},
keywords = {algorithm, public policy, artificial intelligence, social impact, government},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534174,
author = {Loreggia, Andrea and Mattei, Nicholas and Rahgooy, Taher and Rossi, Francesca and Srivastava, Biplav and Venable, Kristen Brent},
title = {Making Human-Like Moral Decisions},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534174},
doi = {10.1145/3514094.3534174},
abstract = {Many real-life scenarios require humans to make difficult trade-offs: do we always follow all the traffic rules or do we violate the speed limit in an emergency? In general, how should we account for and balance the ethical values, safety recommendations, and societal norms, when we are trying to achieve a certain objective? To enable effective AI-human collaboration, we must equip AI agents with a model of how humans make such trade-offs in environments where there is not only a goal to be reached, but there are also ethical constraints to be considered and to possibly align with. These ethical constraints could be both deontological rules on actions that should not be performed, or also consequentialist policies that recommend avoiding reaching certain states of the world. Our purpose is to build AI agents that can mimic human behavior in these ethically constrained decision environments, with a long term research goal to use AI to help humans in making better moral judgments and actions. To this end, we propose a computational approach where competing objectives and ethical constraints are orchestrated through a method that leverages a cognitive model of human decision making, called multi-alternative decision field theory (MDFT). Using MDFT, we build an orchestrator, called MDFT-Orchestrator (MDFT-O), that is both general and flexible. We also show experimentally that MDFT-O both generates better decisions than using a heuristic that takes a weighted average of competing policies (WA-O), but also performs better in terms of mimicking human decisions as collected through Amazon Mechanical Turk (AMT). Our methodology is therefore able to faithfully model human decision in ethically constrained decision environments.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {447–454},
numpages = {8},
keywords = {human decision-making process, markov decision processes, orchestration, cognitive model, ethical constraints},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534148,
author = {Lyu, Yiwei and Liang, Paul Pu and Deng, Zihao and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
title = {DIME: Fine-Grained Interpretations of Multimodal Models via Disentangled Local Explanations},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534148},
doi = {10.1145/3514094.3534148},
abstract = {The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {455–467},
numpages = {13},
keywords = {interpretability, multimodal machine learning, visualization, explainability},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534170,
author = {Marchiori Manerba, Marta and Guidotti, Riccardo},
title = {Investigating Debiasing Effects on Classification and Explainability},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534170},
doi = {10.1145/3514094.3534170},
abstract = {During each stage of a dataset creation and development process, harmful biases can be accidentally introduced, leading to models that perpetuates marginalization and discrimination of minorities, as the role of the data used during the training is critical. We propose an evaluation framework that investigates the impact on classification and explainability of bias mitigation preprocessing techniques used to assess data imbalances concerning minorities' representativeness and mitigate the skewed distributions discovered. Our evaluation focuses on assessing fairness, explainability and performance metrics. We analyze the behavior of local model-agnostic explainers on the original and mitigated datasets to examine whether the proxy models learned by the explainability techniques to mimic the black-boxes disproportionately rely on sensitive attributes, demonstrating biases rooted in the explainers. We conduct several experiments about known biased datasets to demonstrate our proposal's novelty and effectiveness for evaluation and bias detection purposes.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {468–478},
numpages = {11},
keywords = {ml evaluation, data equity, bias mitigation, algorithmic bias, fairness in ml, algorithmic auditing, xai},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534177,
author = {McIlroy-Young, Reid and Kleinberg, Jon and Sen, Siddhartha and Barocas, Solon and Anderson, Ashton},
title = {Mimetic Models: Ethical Implications of AI That Acts Like You},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534177},
doi = {10.1145/3514094.3534177},
abstract = {An emerging theme in artificial intelligence research is the creation of models to simulate the decisions and behavior of specific people, in domains including game-playing, text generation, and artistic expression. These models go beyond earlier approaches in the way they are tailored to individuals, and the way they are designed for interaction rather than simply the reproduction of fixed, pre-computed behaviors. We refer to these as mimetic models, and in this paper we develop a framework for characterizing the ethical and social issues raised by their growing availability. Our framework includes a number of distinct scenarios for the use of such models, and considers the impacts on a range of different participants, including the target being modeled, the operator who deploys the model, and the entities that interact with it.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {479–490},
numpages = {12},
keywords = {ethics, generative models, artificial intelligence, mimetic models, machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534175,
author = {Mill, Eleanor and Garn, Wolfgang and Ryman-Tubb, Nick},
title = {Managing Sustainability Tensions in Artificial Intelligence: Insights from Paradox Theory},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534175},
doi = {10.1145/3514094.3534175},
abstract = {This paper offers preliminary reflections on the sustainability tensions present in Artificial Intelligence (AI) and suggests that Paradox Theory, an approach borrowed from the strategic management literature, may help guide scholars towards innovative solutions. The benefits of AI to our society are well documented. Yet those benefits come at environmental and sociological cost, a fact which is often overlooked by mainstream scholars and practitioners. After examining the nascent corpus of literature on the sustainability tensions present in AI, this paper introduces the Accuracy - Energy Paradox and suggests how the principles of paradox theory can guide the AI community to a more sustainable solution.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {491–498},
numpages = {8},
keywords = {sustainable ai, green ai, paradox theory, sustainability of ai},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534143,
author = {Mutlu, Ece \c{C}i\u{g}dem and Yousefi, Niloofar and Ozmen Garibay, Ozlem},
title = {Contrastive Counterfactual Fairness in Algorithmic Decision-Making},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534143},
doi = {10.1145/3514094.3534143},
abstract = {The widespread use of artificial intelligence algorithms and their role in decision-making with consequential decisions for human subjects has resulted in a growing interest in designing AI algorithms accounting for fairness considerations. There have been attempts to account for fairness of AI algorithms without compromising their accuracy to improve poorly designed algorithms that disregard sensitive attributes (e.g., age, race, and gender) at the peril of introducing or increasing bias against specific groups. Although many studies have examined the optimal trade-off between fairness and accuracy, it remains a challenge to understand the sources of unfairness in decision-making and mitigate it effectively. To tackle this problem, researchers have proposed fair causal learning approaches which assist us in modeling cause and effect knowledge structures, discovering bias sources, and refining AI algorithms to make them more transparent and explainable. In this study, we formalize probabilistic interpretations of both contrastive and counterfactual causality as essential features in order to encourage users' trust and to expand the applicability of such automated systems. We use this formalism to define a novel fairness criterion that we call contrastive counterfactual fairness. This paper introduces, to the best of our knowledge, the first probabilistic fairness-aware data augmentation approach that is based on contrastive counterfactual causality. We tested our approach on two well-known fairness-related datasets, UCI Adult and German Credit, and concluded that our proposed method has a promising ability to capture and mitigate unfairness in AI deployment. This model-agnostic approach can be used with any AI model because it is applied in pre-processing.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {499–507},
numpages = {9},
keywords = {fair causal learning, fair data augmentation, contrastive fairness, counterfactual fairness, fair classification},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534165,
author = {Narayanan, Saumik and Yu, Guanghui and Tang, Wei and Ho, Chien-Ju and Yin, Ming},
title = {How Does Predictive Information Affect Human Ethical Preferences?},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534165},
doi = {10.1145/3514094.3534165},
abstract = {Artificial intelligence (AI) has been increasingly involved in decision making in high-stakes domains, including loan applications, employment screening, and assistive clinical decision making. Meanwhile, involving AI in these high-stake decisions has created ethical concerns on how to balance different trade-offs to respect human values. One approach for aligning AIs with human values is to elicit human ethical preferences and incorporate this information in the design of computer systems. In this work, we explore how human ethical preferences are impacted by the information shown to humans during elicitation. In particular, we aim to provide a contrast between verifiable information (e.g., patient demographics or blood test results) and predictive information (e.g., the probability of organ transplant success). Using kidney transplant allocation as a case study, we conduct a randomized experiment to elicit human ethical preferences on scarce resource allocation to understand how human ethical preferences are impacted by the verifiable and predictive information. We find that the presence of predictive information significantly changes how humans take into account other verifiable information in their ethical preferences. We also find that the source of the predictive information (e.g., whether the predictions are made by AI or human doctors) plays a key role in how humans incorporate the predictive information into their own ethical judgements.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {508–517},
numpages = {10},
keywords = {preference elicitation, ethical preference, ai ethics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534176,
author = {Omrani Sabbaghi, Shiva and Caliskan, Aylin},
title = {Measuring Gender Bias in Word Embeddings of Gendered Languages Requires Disentangling Grammatical Gender Signals},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534176},
doi = {10.1145/3514094.3534176},
abstract = {Does the grammatical gender of a language interfere when measuring the semantic gender information captured by its word embeddings? A number of anomalous gender bias measurements in the embeddings of gendered languages suggest this possibility. We demonstrate that word embeddings learn the association between a noun and its grammatical gender in grammatically gendered languages, which can skew social gender bias measurements. Consequently, word embedding post-processing methods are introduced to quantify, disentangle, and evaluate grammatical gender signals. The evaluation is performed on five gendered languages from the Germanic, Romance, and Slavic branches of the Indo-European language family. Our method reduces the strength of grammatical gender signals, which is measured in terms of effect size (Cohen's d ), by a significant average of d = 1.3 for French, German, and Italian, and d = 0.56 for Polish and Spanish. Once grammatical gender is disentangled, the association between over 90% of 10,000 inanimate nouns and their assigned grammatical gender weakens, and cross-lingual bias results from the Word Embedding Association Test (WEAT) become more congruent with country-level implicit bias measurements. The results further suggest that disentangling grammatical gender signals from word embeddings may lead to improvement in semantic machine learning tasks.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {518–531},
numpages = {14},
keywords = {bias, grammatical gender, word embeddings},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534166,
author = {Papakyriakopoulos, Orestis and Tessono, Christelle and Narayanan, Arvind and Kshirsagar, Mihir},
title = {How Algorithms Shape the Distribution of Political Advertising: Case Studies of Facebook, Google, and TikTok},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534166},
doi = {10.1145/3514094.3534166},
abstract = {Online platforms play an increasingly important role in shaping democracy by influencing the distribution of political information to the electorate. In recent years, political campaigns have spent heavily on the platforms' algorithmic tools to target voters with online advertising. While the public interest in understanding how platforms perform the task of shaping the political discourse has never been higher, the efforts of the major platforms to make the necessary disclosures to understand their practices falls woefully short. In this study, we collect and analyze a dataset containing over 800,000 ads and 2.5 million videos about the 2020 U.S. presidential election from Facebook, Google, and TikTok. We conduct the first large scale data analysis of public data to critically evaluate how these platforms amplified or moderated the distribution of political advertisements. We conclude with recommendations for how to improve the disclosures so that the public can hold the platforms and political advertisers accountable.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {532–546},
numpages = {15},
keywords = {accountability, political speech, algorithmic targeting, algorithmic auditing, political advertising, interpretability, regulation},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534127,
author = {Puranik, Bhagyashree and Madhow, Upamanyu and Pedarsani, Ramtin},
title = {A Dynamic Decision-Making Framework Promoting Long-Term Fairness},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534127},
doi = {10.1145/3514094.3534127},
abstract = {With AI-based decisions playing an increasingly consequential role in our society, for example, in our financial and criminal justice systems, there is a great deal of interest in designing algorithms conforming to application-specific notions of fairness. In this work, we ask a complementary question: can AI-based decisions be designed to dynamically influence the evolution of fairness in our society over the long term? To explore this question, we propose a framework for sequential decision-making aimed at dynamically influencing long-term societal fairness, illustrated via the problem of selecting applicants from a pool consisting of two groups, one of which is under-represented. We consider a dynamic model for the composition of the applicant pool, in which admission of more applicants from a group in a given selection round positively reinforces more candidates from the group to participate in future selection rounds. Under such a model, we show the efficacy of the proposed Fair-Greedy selection policy which systematically trades the sum of the scores of the selected applicants ("greedy'') against the deviation of the proportion of selected applicants belonging to a given group from a target proportion ("fair''). In addition to experimenting on synthetic data, we adapt static real-world datasets on law school candidates and credit lending to simulate the dynamics of the composition of the applicant pool. We prove that the applicant pool composition converges to a target proportion set by the decision-maker when score distributions across the groups are identical.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {547–556},
numpages = {10},
keywords = {ai for social equity, fair selection, long-term fairness, positive reinforcement, sequential decision-making},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534181,
author = {Raji, Inioluwa Deborah and Xu, Peggy and Honigsberg, Colleen and Ho, Daniel},
title = {Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534181},
doi = {10.1145/3514094.3534181},
abstract = {Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {557–571},
numpages = {15},
keywords = {policy, society, accountability, algorithms, auditing},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534189,
author = {Rhea, Alene and Markey, Kelsey and D'Arinzo, Lauren and Schellmann, Hilke and Sloane, Mona and Squires, Paul and Stoyanovich, Julia},
title = {Resume Format, LinkedIn URLs and Other Unexpected Influences on AI Personality Prediction in Hiring: Results of an Audit},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534189},
doi = {10.1145/3514094.3534189},
abstract = {Automated hiring systems are among the fastest-developing of all high-stakes AI systems. Among these are algorithmic personality tests that use insights from psychometric testing, and promise to surface personality traits indicative of future success based on job seekers' resumes or social media profiles. We interrogate the reliability of such systems using stability of the outputs they produce, noting that reliability is a necessary, but not a sufficient, condition for validity. We develop a methodology for an external audit of stability of algorithmic personality tests, and instantiate this methodology in an audit of two systems, Humantic AI and Crystal. Rather than challenging or affirming the assumptions made in psychometric testing -- that personality traits are meaningful and measurable constructs, and that they are indicative of future success on the job -- we frame our methodology around testing the underlying assumptions made by the vendors of the algorithmic personality tests themselves.In our audit of Humantic AI and Crystal, we find that both systems show substantial instability on key facets of measurement, and so cannot be considered valid testing instruments. For example, Crystal frequently computes different personality scores if the same resume is given in PDF vs. in raw text, violating the assumption that the output of an algorithmic personality test is stable across job-irrelevant input variations. Among other notable findings is evidence of persistent --- and often incorrect --- data linkage by Humantic AI.An open-source implementation of our auditing methodology, and of the audits of Humantic AI and Crystal, is available at https://github.com/DataResponsibly/hiring-stability-audit.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {572–587},
numpages = {16},
keywords = {hiring, stability, validity, personality, algorithm audit, reliability},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534126,
author = {Robertson, Jake and Stinson, Catherine and Hu, Ting},
title = {A Bio-Inspired Framework for Machine Bias Interpretation},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534126},
doi = {10.1145/3514094.3534126},
abstract = {Machine learning algorithms use the past and the present to predict the future. But when given biased historical data, these algorithms can quickly become discriminatory. The area of machine learning fairness has emerged to detect and de-bias these algorithms, but has received widespread criticism for its one-size-fits-all approach, which allows certain cases of bias to slip through the cracks. In this study, we take a deeper look at the mechanisms by which machine learning algorithms develop harmful bias. We introduce a new method to interpret discriminatory systems, an Evolutionary algorithm for Feature Interaction (EFI), which we apply to several commonly used machine learning algorithms in two real-world problem instances: violent crime and median house price prediction. In the results, we discover several complex forms of bias including the encoding of race through other seemingly unrelated attributes. Ultimately we suggest that more informative interpretation tools such as EFI can be used to not only explain machine learning outcomes, but supplement and improve existing machine bias detection approaches to provide a more robust and in-depth ethical evaluation of machine learning algorithms.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {588–598},
numpages = {11},
keywords = {interpretability, machine bias, feature importance, feature interaction, fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534191,
author = {Saini, Aditya and Prasad, Ranjitha},
title = {Select Wisely and Explain: Active Learning and Probabilistic Local Post-Hoc Explainability},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534191},
doi = {10.1145/3514094.3534191},
abstract = {Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive domains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active learning based locally faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression (GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {599–608},
numpages = {10},
keywords = {Bayesian learning, uncertainty reduction, Gaussian process, acquisition functions, explainable AI},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534135,
author = {Sapiezynski, Piotr and Ghosh, Avijit and Kaplan, Levi and Rieke, Aaron and Mislove, Alan},
title = {Algorithms That "Don't See Color": Measuring Biases in Lookalike and Special Ad Audiences},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534135},
doi = {10.1145/3514094.3534135},
abstract = {Researchers and journalists have repeatedly shown that algorithms commonly used in domains such as credit, employment, healthcare, or criminal justice can have discriminatory effects. Some organizations have tried to mitigate these effects by simply removing sensitive features from an algorithm's inputs. In this paper, we explore the limits of this approach using a unique opportunity. In 2019, Facebook agreed to settle a lawsuit by removing certain sensitive features from inputs of an algorithm that identifies users similar to those provided by an advertiser for ad targeting, making both the modified and unmodified versions of the algorithm available to advertisers. We develop methodologies to measure biases along the lines of gender, age, and race in the audiences created by this modified algorithm, relative to the unmodified one. Our results provide experimental proof that merely removing demographic features from a real-world algorithmic system's inputs can fail to prevent biased outputs. As a result, organizations using algorithms to help mediate access to important life opportunities should consider other approaches to mitigating discriminatory effects.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {609–616},
numpages = {8},
keywords = {process fairness, fairness, online advertising},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534128,
author = {Schemmer, Max and Hemmer, Patrick and Nitsche, Maximilian and K\"{u}hl, Niklas and V\"{o}ssing, Michael},
title = {A Meta-Analysis of the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534128},
doi = {10.1145/3514094.3534128},
abstract = {Research in artificial intelligence (AI)-assisted decision-making is experiencing tremendous growth with a constantly rising number of studies evaluating the effect of AI with and without techniques from the field of explainable AI (XAI) on human decision-making performance. However, as tasks and experimental setups vary due to different objectives, some studies report improved user decision-making performance through XAI, while others report only negligible effects. Therefore, in this article, we present an initial synthesis of existing research on XAI studies using a statistical meta-analysis to derive implications across existing research. We observe a statistically positive impact of XAI on users' performance. Additionally, the first results indicate that human-AI decision-making tends to yield better task performance on text data. However, we find no effect of explanations on users' performance compared to sole AI predictions. Our initial synthesis gives rise to future research investigating the underlying causes and contributes to further developing algorithms that effectively benefit human decision-makers by providing meaningful explanations.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {617–626},
numpages = {10},
keywords = {empirical studies, explainable artificial intelligence, decision-making, meta-analysis},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534161,
author = {Schopmans, Hendrik R.},
title = {From Coded Bias to Existential Threat: Expert Frames and the Epistemic Politics of AI Governance},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534161},
doi = {10.1145/3514094.3534161},
abstract = {While the knowledge produced by experts has been widely recognized to play a salient role in shaping policy on technological issues, the interaction between AI expertise and the evolving AI governance landscape has received little attention thus far. To address this gap, the present paper leverages insights from STS and International Relations to explore how different expert communities have constructed AI as a governance problem. More specifically, it presents the preliminary results of a qualitative frame analysis of 90 policy documents published by experts from industry, civil society, and the research community. The analysis finds that AI expertise is a highly contested field, as experts not only disagree on why AI is problematic and what policies are required, but, more fundamentally, about which artifacts, ideas, and practices make up AI in the first place. The paper proposes that the epistemic disagreements concerning AI have political consequences, as they engender protracted ontological politics that jeopardize the development of effective governance interventions. Against this background, the findings raise critical questions about the prevailing tendency of governance interventions to target the elusive and contested object 'artificial intelligence.'},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {627–640},
numpages = {14},
keywords = {global governance, frame analysis., governance objects, artificial intelligence, experts, epistemic community},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534186,
author = {Seymour, William and Van Kleek, Max and Binns, Reuben and Murray-Rust, Dave},
title = {Respect as a Lens for the Design of AI Systems},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534186},
doi = {10.1145/3514094.3534186},
abstract = {Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn't technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {641–652},
numpages = {12},
keywords = {ethical design, respect, AI systems},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534163,
author = {Shea-Blymyer, Colin and Abbas, Houssam},
title = {Generating Deontic Obligations From Utility-Maximizing Systems},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534163},
doi = {10.1145/3514094.3534163},
abstract = {This work gives a logical characterization of the (ethical and social) obligations of an agent trained with Reinforcement Learning (RL). An RL agent takes actions by following a utility-maximizing policy. We maintain that the choice of utility function embeds ethical and social values implicitly, and that it is necessary to make these values explicit. This work provides a basis for doing so. First, we propose a probabilistic deontic logic that is suited for formally specifying the obligations of a stochastic system, including its ethical obligations. We prove some useful validities about this logic, and how its semantics are compatible with those of Markov Decision Processes (MDPs). Second, we show that model checking allows us to prove that an agent has a given obligation to bring about some state of affairs - meaning that by acting optimally, it is seeking to reach that state of affairs. We develop a model checker for our logic against MDPs. Third, we observe that it is useful for a system designer to obtain a logical characterization of her system's obligations, which is potentially more interpretable and helpful in debugging than the expression of a utility function. Enumerating all the obligations of an agent is impractical, so we propose a Bayesian optimization routine that learns to generate a system's obligations that the system designer deems interesting. We implement the model checking and Bayesian optimization routines, and demonstrate their effectiveness with an initial pilot study. This work provides a rigorous method to characterize utility-maximizing agents in terms of the (ethical and social) obligations that they implicitly seek to satisfy.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {653–663},
numpages = {11},
keywords = {machine ethics, deontic logic, model checking, explainability, normative systems},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534194,
author = {Shimao, Hajime and Khern-am-nuai, Warut and Kannan, Karthik and Cohen, Maxime C.},
title = {Strategic Best Response Fairness in Fair Machine Learning},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534194},
doi = {10.1145/3514094.3534194},
abstract = {While artificial intelligence (AI) and machine learning (ML) have been increasingly used for decision-making, issues related to discrimination in AI/ML have become prominent. While several fair algorithms are proposed to alleviate these discrimination issues, most of them provide fairness by imposing constraints to eliminate disparity in prediction results. However, the use of these fair algorithms may change the behavior of prediction subjects. As such, even though the disparity in prediction results might be removed by fair algorithms, behavioral responses to the use of fair algorithms can still create disparity in behavior which may persist across different groups of prediction subjects. To study this issue, we define a notion called "strategic best-response fairness" (SBR-fair). It is defined in a context that includes different groups of prediction subjects who are ex-ante identical in terms of abilities and conditional payoffs. We utilize a game-theoretic model to investigate whether different types of fair algorithms lead to identical equilibrium behaviors among different groups of prediction subjects. If yes, such an algorithm is considered SBR-fair. We then demonstrate that many existing fair algorithms are not SBR-fair. As a result, implementing these algorithms may impose fairness on prediction results but actually induce disparity between privileged and unprivileged individuals in the long run.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {664},
numpages = {1},
keywords = {strategic best response, fair machine learning, game-theoretic model},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534197,
author = {Siapka, Anastasia},
title = {Towards a Feminist Metaethics of AI},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534197},
doi = {10.1145/3514094.3534197},
abstract = {The proliferation of Artificial Intelligence (AI) has sparked an overwhelming number of AI ethics guidelines, boards and codes of conduct. These outputs primarily analyse competing theories, principles and values for AI development and deployment. However, as a series of recent problematic incidents about AI ethics/ethicists demonstrate, this orientation is insufficient. Before proceeding to evaluate other professions, AI ethicists should critically evaluate their own; yet, such an evaluation should be more explicitly and systematically undertaken in the literature. I argue that these insufficiencies could be mitigated by developing a research agenda for a feminist metaethics of AI. Contrary to traditional metaethics, which reflects on the nature of morality and moral judgements in a non-normative way, feminist metaethics expands its scope to ask not only what ethics is but also what our engagement with it should be like. Applying this perspective to the context of AI, I suggest that a feminist metaethics of AI would examine: (i) the continuity between theory and action in AI ethics; (ii) the real-life effects of AI ethics; (iii) the role and profile of those involved in AI ethics; and (iv) the effects of AI on power relations through methods that pay attention to context, emotions and narrative.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {665–674},
numpages = {10},
keywords = {artificial intelligence, metaethics, ethics-washing, AI ethics, feminist philosophy},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534132,
author = {Simonetto, Andrea and Notarnicola, Ivano},
title = {Achievement and Fragility of Long-Term Equitability},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534132},
doi = {10.1145/3514094.3534132},
abstract = {Equipping current decision-making tools with notions of fairness, equitability, or other ethically motivated outcomes, is one of the top priorities in recent research efforts in machine learning, AI, and optimization. In this paper, we investigate how to allocate limited resources to locally interacting communities in a way to maximize a pertinent notion of equitability. In particular, we look at the dynamic setting where the allocation is repeated across multiple periods (e.g., yearly), the local communities evolve in the meantime (driven by the provided allocation), and the allocations are modulated by feedback coming from the communities themselves. We employ recent mathematical tools stemming from data-driven feedback online optimization, by which communities can learn their (possibly unknown) evolution, satisfaction, as well as they can share information with the deciding bodies. We design dynamic policies that converge to an allocation that maximize equitability in the long term. We further demonstrate our model and methodology with realistic examples of healthcare and education subsidies design in Sub-Saharian countries. One of the key empirical takeaways from our setting is that long-term equitability is fragile, in the sense that it can be easily lost when deciding bodies weigh in other factors (e.g., equality in allocation) in the allocation strategy. Moreover, a naive compromise, while not providing significant advantage to the communities, can promote inequality in social outcomes.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {675–685},
numpages = {11},
keywords = {optimization, dynamical systems, subsidies design, equitability, fairness},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534198,
author = {Singh, Harvineet and Joshi, Shalmali and Doshi-Velez, Finale and Lakkaraju, Himabindu},
title = {Towards Robust Off-Policy Evaluation via Human Inputs},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534198},
doi = {10.1145/3514094.3534198},
abstract = {Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies in high-stakes domains such as healthcare, where direct deployment is often infeasible, unethical, or expensive. When deployment environments are expected to undergo changes (that is, dataset shifts), it is important for OPE methods to perform robust evaluation of the policies amidst such changes. Existing approaches consider robustness against a large class of shifts that can arbitrarily change any observable property of the environment. This often results in highly pessimistic estimates of the utilities, thereby invalidating policies that might have been useful in deployment. In this work, we address the aforementioned problem by investigating how domain knowledge can help provide more realistic estimates of the utilities of policies. We leverage human inputs on which aspects of the environments may plausibly change, and adapt the OPE methods to only consider shifts on these aspects. Specifically, we propose a novel framework, Robust OPE (ROPE), which considers shifts on a subset of covariates in the data based on user inputs, and estimates worst-case utility under these shifts. We then develop computationally efficient algorithms for OPE that are robust to the aforementioned shifts for contextual bandits and Markov decision processes. We also theoretically analyze the sample complexity of these algorithms. Extensive experimentation with synthetic and real world datasets from the healthcare domain demonstrates that our approach not only captures realistic dataset shifts accurately, but also results in less pessimistic policy evaluations.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {686–699},
numpages = {14},
keywords = {dataset shift, adversarial machine learning, policy evaluation, human-in-the-loop, robust learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534178,
author = {Skorupa Parolin, Erick and Hosseini, MohammadSaleh and Hu, Yibo and Khan, Latifur and Brandt, Patrick T. and Osorio, Javier and D'Orazio, Vito},
title = {Multi-CoPED: A Multilingual Multi-Task Approach for Coding Political Event Data on Conflict and Mediation Domain},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534178},
doi = {10.1145/3514094.3534178},
abstract = {Political and social scientists monitor, analyze and predict political unrest and violence, preventing (or mitigating) harm, and promoting the management of global conflict. They do so using event coder systems, which extract structured representations from news articles to design forecast models and event-driven continuous monitoring systems. Existing methods rely on expensive manual annotated dictionaries and do not support multilingual settings. To advance the global conflict management, we propose a novel model, Multi-CoPED (Multilingual Multi-Task Learning BERT for Coding Political Event Data), by exploiting multi-task learning and state-of-the-art language models for coding multilingual political events. This eliminates the need for expensive dictionaries by leveraging BERT models' contextual knowledge through transfer learning. The multilingual experiments demonstrate the superiority of Multi-CoPED over existing event coders, improving the absolute macro-averaged F1-scores by 23.3% and 30.7% for coding events in English and Spanish corpus, respectively. We believe that such expressive performance improvements can help to reduce harms to people at risk of violence.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {700–711},
numpages = {12},
keywords = {event coding, artificial intelligence and geopolitics, social conflict, transfer learning, political conflict, natural language processing},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534185,
author = {Sullivan, Emily and Verreault-Julien, Philippe},
title = {From Explanation to Recommendation: Ethical Standards for Algorithmic Recourse},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534185},
doi = {10.1145/3514094.3534185},
abstract = {People are increasingly subject to algorithmic decisions, and it is generally agreed that end-users should be provided an explanation or rationale for these decisions. There are different purposes that explanations can have, such as increasing user trust in the system or allowing users to contest the decision. One specific purpose that is gaining more traction is algorithmic recourse. We first propose that recourse should be viewed as a recommendation problem, not an explanation problem. Then, we argue that the capability approach provides plausible and fruitful ethical standards for recourse. We illustrate by considering the case of diversity constraints on algorithmic recourse. Finally, we discuss the significance and implications of adopting the capability approach for algorithmic recourse research.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {712–722},
numpages = {11},
keywords = {explainable AI, counterfactuals, algorithmic recourse, recommendations, capability approach, diversity},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534202,
author = {Tong, Xin and Li, Yixuan and Li, Jiayi and Bei, Rongqi and Zhang, Luyao},
title = {What Are People Talking about in #BackLivesMatter and #StopAsianHate? Exploring and Categorizing Twitter Topics Emerged in Online Social Movements through the Latent Dirichlet Allocation Model},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534202},
doi = {10.1145/3514094.3534202},
abstract = {Minority groups have been using social media to organize social movements that create profound social impacts. Black Lives Matter (BLM) and Stop Asian Hate (SAH) are two successful social movements that have spread on Twitter that promote protests and activities against racism and increase the public's awareness of other social challenges that minority groups face. However, previous studies have mostly conducted qualitative analyses of tweets or interviews with users, which may not comprehensively and validly represent all tweets. Very few studies have explored the Twitter topics within BLM and SAH dialogs in a rigorous, quantified and data-centered approach. Therefore, in this research, we adopted a mixed-methods approach to comprehensively analyze BLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation model to understand the top high-level words and topics and (2) open-coding analysis to identify specific themes across the tweets. We collected more than one million tweets with the #blacklivesmatter and #stopasianhate hashtags and compared their topics. Our findings revealed that the tweets discussed a variety of influential topics in depth, and social justice, social movements, and emotional sentiments were common topics in both movements, though with unique subtopics for each movement. Our study contributes to the topic analysis of social movements on social media platforms in particular and the literature on the interplay of AI, ethics, and society in general.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {723–738},
numpages = {16},
keywords = {society, #stopasianhate, social movements, natural language processing, ai, open-coding analysis, dirichlet allocation model, topic analysis, ethics, #blacklivesmatter, twitter},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534133,
author = {Triantafyllou, Stelios and Singla, Adish and Radanovic, Goran},
title = {Actual Causality and Responsibility Attribution in Decentralized Partially Observable Markov Decision Processes},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534133},
doi = {10.1145/3514094.3534133},
abstract = {Actual causality and a closely related concept of responsibility attribution are central to accountable decision making. Actual causality focuses on specific outcomes and aims to identify decisions (actions) that were critical in realizing an outcome of interest. Responsibility attribution is complementary and aims to identify the extent to which decision makers (agents) are responsible for this outcome. In this paper, we study these concepts under a widely used framework for multi-agent sequential decision making under uncertainty: decentralized partially observable Markov decision processes (Dec-POMDPs). Following recent works in RL that show correspondence between POMDPs and Structural Causal Models (SCMs), we first establish a connection between Dec-POMDPs and SCMs. This connection enables us to utilize a language for describing actual causality from prior work and study existing definitions of actual causality in Dec-POMDPs. Given that some of the well-known definitions may lead to counter-intuitive actual causes, we introduce a novel definition that more explicitly accounts for causal dependencies between agents' actions. We then turn to responsibility attribution based on actual causality, where we argue that in ascribing responsibility to an agent it is important to consider both the number of actual causes in which the agent participates, as well as its ability to manipulate its own degree of responsibility. Motivated by these arguments we introduce a family of responsibility attribution methods that extends prior work, while accounting for the aforementioned considerations. Finally, through a simulation-based experiment, we compare different definitions of actual causality and responsibility attribution methods. The empirical results demonstrate the qualitative difference between the considered definitions of actual causality and their impact on attributed responsibility.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {739–752},
numpages = {14},
keywords = {actual causality, multi-agent systems, responsibility attribution},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534168,
author = {Unruh, Charlotte Franziska and Haid, Charlotte and Johannes, Fottner and B\"{u}the, Tim},
title = {Human Autonomy in Algorithmic Management},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534168},
doi = {10.1145/3514094.3534168},
abstract = {Algorithmic management tools support or replace managerial decision making in areas such as task allocation, shift planning, or team formation. These tools can have a significant impact on the lives of workers. In this paper, we contribute to the emerging literature on the ethics of algorithmic management by developing a conceptual framework for autonomy at work. Further, we use this framework to discuss risks and opportunities for autonomy in the context of work decision algorithms in Industry 4.0.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {753–762},
numpages = {10},
keywords = {algorithmic management, autonomy, ai ethics, industry 4.0, work decision algorithms},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534150,
author = {Vodrahalli, Kailas and Daneshjou, Roxana and Gerstenberg, Tobias and Zou, James},
title = {Do Humans Trust Advice More If It Comes from AI? An Analysis of Human-AI Interactions},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534150},
doi = {10.1145/3514094.3534150},
abstract = {In decision support applications of AI, the AI algorithm's output is framed as a suggestion to a human user. The user may ignore this advice or take it into consideration to modify their decision. With the increasing prevalence of such human-AI interactions, it is important to understand how users react to AI advice. In this paper, we recruited over 1100 crowdworkers to characterize how humans use AI suggestions relative to equivalent suggestions from a group of peer humans across several experimental settings. We find that participants' beliefs about how human versus AI performance on a given task affects whether they heed the advice. When participants do heed the advice, they use it similarly for human and AI suggestions. Based on these results, we propose a two-stage, "activation-integration" model for human behavior and use it to characterize the factors that affect human-AI interactions.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {763–777},
numpages = {15},
keywords = {ai trust, artificial intelligence, human interaction with ai, ai advice, human-in-the-loop},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534156,
author = {Wan, Charles and Belo, Rodrigo and Zejnilovic, Leid},
title = {Explainability's Gain is Optimality's Loss? How Explanations Bias Decision-Making},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534156},
doi = {10.1145/3514094.3534156},
abstract = {Decisions in organizations are about evaluating alternatives and choosing the one that would best serve organizational goals. To the extent that the evaluation of alternatives could be formulated as a predictive task with appropriate metrics, machine learning algorithms are increasingly being used to improve the efficiency of the process. Explanations help to facilitate communication between the algorithm and the human decision-maker, making it easier for the latter to interpret and make decisions on the basis of predictions by the former. Feature-based explanations' semantics of causal models, however, induce leakage from the decision-maker's prior beliefs. Our findings from a field experiment demonstrate empirically how this leads to confirmation bias and disparate impact on the decision-maker's confidence in the predictions. Such differences can lead to sub-optimal and biased decision outcomes.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {778–787},
numpages = {10},
keywords = {confirmation bias, human causal reasoning, human-algorithm communication, semantics of explanations, decision-making, explanations, bias},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534138,
author = {Winecoff, Amy A. and Watkins, Elizabeth Anne},
title = {Artificial Concepts of Artificial Intelligence: Institutional Compliance and Resistance in AI Startups},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534138},
doi = {10.1145/3514094.3534138},
abstract = {Scholars and industry practitioners have debated how to best develop interventions for ethical artificial intelligence (AI). Such interventions recommend that companies building and using AI tools change their technical practices, but fail to wrangle with critical questions about the organizational and institutional context in which AI is developed. In this paper, we contribute descriptive research around the life of "AI" as a discursive concept and organizational practice in an understudied sphere--emerging AI startups--and with a focus on extra-organizational pressures faced by entrepreneurs. Leveraging a theoretical lens for how organizations change, we conducted semi-structured interviews with 23 entrepreneurs working at early-stage AI startups. We find that actors within startups both conform to and resist institutional pressures. Our analysis identifies a central tension for AI entrepreneurs: they often valued scientific integrity and methodological rigor; however, influential external stakeholders either lacked the technical knowledge to appreciate entrepreneurs' emphasis on rigor or were more focused on business priorities. As a result, entrepreneurs adopted hyped marketing messages about AI that diverged from their scientific values, but attempted to preserve their legitimacy internally. Institutional pressures and organizational constraints also influenced entrepreneurs' modeling practices and their response to actual or impending regulation. We conclude with a discussion for how such pressures could be used as leverage for effective interventions towards building ethical AI.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {788–799},
numpages = {12},
keywords = {ethical systems, artificial intelligence, industry practice, organizational theory, qualitative methods},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534136,
author = {Wolfe, Robert and Caliskan, Aylin},
title = {American == White in Multimodal Language-and-Image AI},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534136},
doi = {10.1145/3514094.3534136},
abstract = {Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP, are evaluated for evidence of a bias previously observed in social and experimental psychology: equating American identity with being White. Embedding association tests (EATs) using standardized images of self-identified Asian, Black, Latina/o, and White individuals from the Chicago Face Database (CFD) reveal that White individuals are more associated with collective in-group words than are Asian, Black, or Latina/o individuals, with effect sizes &gt;.4 for White vs. Asian comparisons across all models. In assessments of three core aspects of American identity reported by social psychologists, single-category EATs reveal that images of White individuals are more associated with patriotism and with being born in America, but that, consistent with prior findings in psychology, White individuals are associated with being less likely to treat people of all races and backgrounds equally. Additional tests reveal that the number of images of Black individuals returned by an image ranking task is more strongly correlated with state-level implicit bias scores for White individuals (Pearson's ρ=.63 in CLIP, ρ=.69 in BLIP) than are state demographics (ρ=.60), suggesting a relationship between regional prototypicality and implicit bias. Three downstream machine learning tasks demonstrate biases associating American with White. In a visual question answering task using BLIP, 97% of White individuals are identified as American, compared to only 3% of Asian individuals. When asked in what state the individual depicted lives in, the model responds China 53% of the time for Asian individuals, but always with an American state for White individuals. In an image captioning task, BLIP remarks upon the race of Asian individuals as much as 36% of the time, and the race of Black individuals as much as 18% of the time, but never remarks upon race for White individuals. Finally, when provided with an initialization image of individuals from the CFD and the text "an American person," a synthetic image generator (VQGAN) using the text-based guidance of CLIP consistently lightens the skin tone of individuals of all races (by 35% for Black individuals, based on mean pixel brightness), and generates output images of White individuals with blonde hair. The results indicate that societal biases equating American identity with being White are learned by multimodal language-and-image AI, and that these biases propagate to downstream applications of such models.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {800–812},
numpages = {13},
keywords = {multimodal models, racial bias, visual semantics, bias in ai},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534153,
author = {Yang, Yu and Gupta, Aayush and Feng, Jianwei and Singhal, Prateek and Yadav, Vivek and Wu, Yue and Natarajan, Pradeep and Hedau, Varsha and Joo, Jungseock},
title = {Enhancing Fairness in Face Detection in Computer Vision Systems by Demographic Bias Mitigation},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534153},
doi = {10.1145/3514094.3534153},
abstract = {Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {813–822},
numpages = {10},
keywords = {bias measurement and mitigation, fairness in computer vision, face detection bias},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534130,
author = {Yew, Rui-Jie and Hadfield-Menell, Dylan},
title = {A Penalty Default Approach to Preemptive Harm Disclosure and Mitigation for AI Systems},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534130},
doi = {10.1145/3514094.3534130},
abstract = {As AI industry matures, it is important to ensure that the organizations developing these systems have sufficient incentives to identify and mitigate risks and harm. Unfortunately, the profit motive is often misaligned with this goal. Successful work to identify or reduce risk rarely has direct tangible benefits. In this paper, we consider the use of regulatory penalty defaults as a way to counter these perverse incentives. A regulatory penalty default regime consists of two parts: a regulatory penalty default and a mechanism to bargain around the default. The regulatory penalty default induces private actors to research and mitigate potential harms in order to limit liability, making the benefits of risk mitigation tangible. The bargaining mechanism provides incentives for companies to go beyond achieving a prescriptive threshold of compliance in creating a compelling case for escape from the default. With a focus on the policy landscape in the United States, we propose and discuss potential regulatory penalty default regimes for AI systems. For each of our proposals, we also discuss accompanying regulatory pathways for the bargaining process. While regulatory penalty default regimes are not a panacea (we discuss several drawbacks of the proposed methods), they are an important tool to consider in the regulation of AI systems.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {823–830},
numpages = {8},
keywords = {computing and society, technology policy, artificial intelligence law, penalty defaults},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534169,
author = {Yik, William and Serafini, Limnanthes and Lindsey, Timothy and Monta\~{n}ez, George D.},
title = {Identifying Bias in Data Using Two-Distribution Hypothesis Tests},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534169},
doi = {10.1145/3514094.3534169},
abstract = {As machine learning models become more widely used in important decision-making processes, the need for identifying and mitigating potential sources of bias has increased substantially. Using two-distribution (specified complexity) hypothesis tests, we identify biases in training data with respect to proposed distributions and without the need to train a model, distinguishing our methods from common output-based fairness tests. Furthermore, our methods allow us to return a "closest plausible explanation" for a given dataset, potentially revealing underlying biases in the processes that generated them. We also show that a binomial variation of this hypothesis test could be used to identify bias in certain directions, or towards certain outcomes, and again return a closest plausible explanation. The benefits of this binomial variation are compared with other hypothesis tests, including the exact binomial. Lastly, potential industrial applications of our methods are shown using two real-world datasets.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {831–844},
numpages = {14},
keywords = {hypothesis testing, fairness, data analysis, bias, statistics, machine learning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534129,
author = {Zhan, Xiao and Sarkadi, Stefan and Criado, Natalia and Such, Jose},
title = {A Model for Governing Information Sharing in Smart Assistants},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534129},
doi = {10.1145/3514094.3534129},
abstract = {Smart Personal Assistants (SPAs), such as Amazon Alexa, Google Assistant and Apple Siri, leverage different AI techniques to provide convenient help and assistance to users. However, inappropriate information sharing decisions can lead SPAs to incorrectly disclose user information to undesired parties, or mistakenly block their reasonable access in specific scenarios to desired parties. In fact, reports about privacy violations in SPAs and associated user concerns are well known and understood in the related literature. It is difficult for SPAs to automatically decide how data should be shared with respect to the privacy preferences of the users. We argue norms, which are regarded as shared standards of acceptable behaviour of groups and/or individuals, can be used to govern and reason about the best course of action of SPAs with regards to information sharing, and our work is the first to propose a practical model to address the above issues and govern SPAs based on normative systems and the contextual integrity theory of privacy. We evaluated the performance of the model using a real dataset of user preferences for privacy in SPAs and the results showed a very marked and significant improvement in understanding user preferences and making the right decisions with respect to data sharing.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {845–855},
numpages = {11},
keywords = {voice assistants, smart personal assistants, personal data, privacy},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534179,
author = {Zhang, Baobao},
title = {No Rage Against the Machines: Threat of Automation Does Not Change Policy Preferences},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534179},
doi = {10.1145/3514094.3534179},
abstract = {Labor-saving technology has already decreased employment opportunities for middle-skill workers. Experts anticipate that advances in AI and robotics will cause even more significant disruptions in the labor market over the next two decades. This paper presents three experimental studies that investigate how this profound economic change could affect mass politics. Recent observational studies suggest that workers' exposure to automation risk predicts their support not only for redistribution but also for right-wing populist policies and candidates. Other observational studies, including my own, find that workers underestimate the impact of automation on their job security. Misdirected blame towards immigrants and workers in foreign countries, rather than concerns about workplace automation, could be driving support for right-wing populism. To correct American workers' beliefs about the threats to their jobs, I conducted three survey experiments in which I informed workers about the existent and future impact of workplace automation. While these informational treatments convinced workers that automation threatens American jobs, they failed to change respondents' preferences on welfare, immigration, and trade policies. My research finds that raising awareness about workplace automation did not decrease opposition to globalization or increase support for policies that will prepare workers for future technological disruptions.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {856–866},
numpages = {11},
keywords = {artificial intelligence and the future of work, public opinion, automation, political economy},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534171,
author = {Zhang, Kang and Shinden, Hiroaki and Mutsuro, Tatsuki and Suzuki, Einoshin},
title = {Judging Instinct Exploitation in Statistical Data Explanations Based on Word Embedding},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534171},
doi = {10.1145/3514094.3534171},
abstract = {This paper proposes 18 types of statistical data explanations and three kinds of procedures to investigate credibility in unethical and biased explanations due to exploitation of the 10 instincts proposed by Rosling et al. The explanation "women have lower math scores than men'' accompanied with the averages and the distributions of their scores is an example of such an explanation, as it exploits the gap instinct, i.e., our tendency to divide all kinds of things into two distinct and often conflicting groups. It becomes much less credible if we replace the word "math'' with "English'', even if we keep the data as they are, as the exploitation seems to fail. Our judging procedures are based on phrase embedding and carefully designed comparisons to judge the credibility. The results of our experiments comparing the 18 types with their variants show promising results and clues for further developments.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {867–879},
numpages = {13},
keywords = {exploitation of thinking traits, text classification, ai and ethics, word embedding},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inproceedings{10.1145/3514094.3534200,
author = {Zilka, Miri and Sargeant, Holli and Weller, Adrian},
title = {Transparency, Governance and Regulation of Algorithmic Tools Deployed in the Criminal Justice System: A UK Case Study},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534200},
doi = {10.1145/3514094.3534200},
abstract = {We present a survey of tools used in the criminal justice system in the UK in three categories: data infrastructure, data analysis, and risk prediction. Many tools are currently in deployment, offering potential benefits, including improved efficiency and consistency. However, there are also important concerns. Transparent information about these tools, their purpose, how they are used, and by whom is difficult to obtain. Even when information is available, it is often insufficient to enable a satisfactory evaluation. More work is needed to establish governance mechanisms to ensure that tools are deployed in a transparent, safe and ethical way. We call for more engagement with stakeholders and greater documentation of the intended goal of a tool, how it will achieve this goal compared to other options, and how it will be monitored in deployment. We highlight additional points to consider when evaluating the trustworthiness of deployed tools and make concrete proposals for policy.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {880–889},
numpages = {10},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

